{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649dd4d4-14f5-4120-b466-ec497f1d7d25",
   "metadata": {},
   "source": [
    "Q:1  what is logistic regression? and when it is used?\n",
    "Q:2  what is sigmoid function why it is use in logistic regression?\n",
    "Q:3  how does logistic regression differ from linear regression?\n",
    "Q:4  what does the output of logictic regression represent?\n",
    "q:5  what are the assumptions of logistic regression?\n",
    "Q:6  what is multicolinearnity and how does affect logistic regression?\n",
    "Q:7  how do you interpret model cofficients in logistic regression?\n",
    "Q:8  how do you handle imbalanced data in logistic regresion?\n",
    "Q:9  what is a decision tree?\n",
    "Q:10 how doea a decision tree makes a prediction\n",
    "Q:11 what are the key components of a decision tree?\n",
    "Q:12 what is difference bet classification and regression trree?\n",
    "Q:13 what are leaf node and internal nodes?\n",
    "Q:14 what are gini impurity and entrophy?\n",
    "Q:15 what is overfitting in decison treeand how can it be prevented?\n",
    "Q:16 what is pruning ? and why it is neccesseary?\n",
    "Q:17 what are the stoping criteria for a decision tree ?\n",
    "Q:18 what is the role of max dept ,minimum samples split and min samples leaf?\n",
    "Q:19 how does a decison tree handles missing values?\n",
    "Q:20 what is diff bet surpervised and unsupervised machine learning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41c258-adee-4c83-a06b-95255620c487",
   "metadata": {},
   "source": [
    "## Q1. What is logistic regression? And when is it used?\n",
    "- Logistic Regression is a supervised learning algorithm used primarily for binary classification tasks. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability that a given input point belongs to a particular class (e.g., yes/no, true/false, spam/not spam).\n",
    "It outputs probabilities using the sigmoid function, and based on a threshold (commonly 0.5), it classifies the instance into one of the two categories.\n",
    "When it is used:\n",
    "\n",
    "- Email spam detection\n",
    "\n",
    "- Predicting whether a customer will churn\n",
    "\n",
    "- Disease diagnosis (e.g., cancer detection)\n",
    "\n",
    "- Credit scoring (loan approval yes/no)\n",
    "## Q2. What is the sigmoid function and why is it used in logistic regression?\n",
    "\n",
    "- The sigmoid function (also known as the logistic function) is defined as:\n",
    "Ïƒ(z) = 1 / (1 + e^(-z))\n",
    "\n",
    "It maps any real-valued number into a range between 0 and 1, which is ideal for representing probabilities.\n",
    "\n",
    "- # Why it's used:\n",
    "- It transforms the output of the linear equation (wx + b) into a probability score.\n",
    "- This probability can then be thresholded to classify into one of the two classes.\n",
    "- Its derivative is easy to compute, which is useful for gradient-based optimization.\n",
    "\n",
    "## Q3. How does logistic regression differ from linear regression?\n",
    "\n",
    "| Feature        | Linear Regression             | Logistic Regression                       |\n",
    "| -------------- | ----------------------------- | ----------------------------------------- |\n",
    "| Output         | Continuous value              | Probability between 0 and 1               |\n",
    "| Used for       | Regression tasks              | Classification tasks                      |\n",
    "| Function       | Linear function: `y = wx + b` | Sigmoid function: `Ïƒ(wx + b)`             |\n",
    "| Loss Function  | Mean Squared Error (MSE)      | Log Loss (Binary Cross-Entropy)           |\n",
    "| Interpretation | Predicts real numbers         | Predicts probability for class membership |\n",
    "\n",
    "\n",
    "# Q4. What does the output of logistic regression represent?\n",
    "\n",
    "- The output of logistic regression is a probability value between 0 and 1. It represents the likelihood that a given input belongs to the positive class (usually labeled as 1). This probability is derived using the sigmoid function.\n",
    "\n",
    "If the output is:\n",
    "\n",
    "- 0.85, the model is 85% confident that the sample belongs to class 1.\n",
    "- A threshold (e.g., 0.5) is then used to decide the final class.\n",
    "\n",
    "# Q5. What are the assumptions of logistic regression?\n",
    "- Linearity of independent variables and log-odds: Logistic regression assumes a linear relationship between the input features and the log-odds of the outcome.\n",
    "\n",
    "- No multicollinearity: Independent variables should not be highly correlated.\n",
    "\n",
    "- Independent observations: Data points should be independent of each other.\n",
    "\n",
    "- Large sample size: Performs better with a large number of observations.\n",
    "\n",
    "- Binary output (for binary logistic regression): Only two possible outcomes\n",
    "\n",
    "# Q6. What is multicollinearity and how does it affect logistic regression?\n",
    "- Multicollinearity occurs when two or more predictor variables are highly correlated. This can cause problems such as:\n",
    "\n",
    "Unstable coefficient estimates\n",
    "\n",
    "Inflated standard errors\n",
    "\n",
    "Difficulty in determining the effect of each independent variable\n",
    "\n",
    "- In logistic regression, multicollinearity makes it hard to interpret coefficients and can reduce the predictive power. To detect it, you can use        the Variance Inflation Factor (VIF), and to fix it, you can:\n",
    "\n",
    "- Remove highly correlated predictors\n",
    "\n",
    "- Use dimensionality reduction (like PCA)\n",
    "\n",
    " - Apply regularization (L1 or L2)\n",
    "# Q7. How do you interpret model coefficients in logistic regression?\n",
    "- Each coefficient in logistic regression represents the change in the log-odds of the dependent variable for a one-unit increase in the predictor, holding all other variables constant.\n",
    "\n",
    "If Î² is the coefficient for variable X:\n",
    "\n",
    "Positive Î² â†’ increases the log-odds of the outcome (probability increases)\n",
    "\n",
    "Negative Î² â†’ decreases the log-odds (probability decreases)\n",
    "\n",
    "To interpret in terms of odds:\n",
    "\n",
    "OddsÂ Ratio\n",
    "=\n",
    "ð‘’\n",
    "ð›½\n",
    "OddsÂ Ratio=e \n",
    "Î²\n",
    " \n",
    "If e^Î² = 2, it means the odds double for each unit increase in X.\n",
    "\n",
    "# How do you handle imbalanced data in logistic regression?\n",
    "\n",
    "- Imbalanced data occurs when one class significantly outweighs the other.\n",
    "\n",
    "Solutions include:\n",
    "\n",
    "- Resampling:\n",
    "  -  Oversampling the minority class (e.g., SMOTE)\n",
    "  -  Undersampling the majority class\n",
    "  -  Class weights: Use class_weight='balanced' in scikit-learn\n",
    "  -  Anomaly detection: Treat the rare class as an anomaly\n",
    "  -  Evaluation Metrics: Use precision, recall, F1-score, and AUC instead of accuracy\n",
    "  -  Threshold tuning: Adjust the decision threshold to favor the minority class\n",
    "\n",
    "# Q9. What is a decision tree?\n",
    "- A Decision Tree is a supervised machine learning model used for both classification and regression tasks. It splits the data into branches using  conditions on input features, eventually reaching a decision at a leaf node.\n",
    "Each node in the tree asks a question based on a feature; the tree grows by answering these questions and routing the data down paths until a final prediction is made\n",
    "\n",
    "# Q10. How does a decision tree make a prediction?\n",
    "- A decision tree makes a prediction by:\n",
    "Starting at the root node\n",
    "Evaluating the condition (e.g., \"Is age > 50?\")\n",
    "Moving left or right based on the answer\n",
    "Repeating this process until it reaches a leaf node, which contains the prediction (class label or value)\n",
    "\n",
    "\n",
    "# Q11. What are the key components of a decision tree?\n",
    "- Root Node: The topmost node; represents the best feature to split the data\n",
    "\n",
    "Internal/Decision Nodes: Nodes where the data is further split\n",
    "\n",
    "Leaf Nodes: Terminal nodes that hold the prediction\n",
    "\n",
    "Edges/Branches: Arrows that connect nodes based on decisions\n",
    "\n",
    "\n",
    "# Q12. What is the difference between classification and regression trees?\n",
    "\n",
    "| Feature            | Classification Tree        | Regression Tree                 |\n",
    "| ------------------ | -------------------------- | ------------------------------- |\n",
    "| Output             | Class label (e.g., Yes/No) | Continuous value (e.g., salary) |\n",
    "| Splitting Criteria | Gini Index, Entropy        | MSE, MAE                        |\n",
    "| Leaf Nodes         | Class labels               | Numerical values                |\n",
    "\n",
    "\n",
    "\n",
    "# Q13. What are leaf nodes and internal nodes?\n",
    "- Internal Nodes: These are decision points that evaluate conditions on features (e.g., age > 30).\n",
    "\n",
    "Leaf Nodes: Final nodes where a decision or prediction is made. They do not split further.\n",
    "\n",
    "# Q14. What are Gini impurity and entropy?\n",
    "- oth are metrics used to measure impurity or disorder in a node when building classification trees.\n",
    "\n",
    "Gini Impurity: Measures probability of incorrect classification:\n",
    "   Gini = 1 - Î£(p_i)^2,\n",
    "Entropy: Measures information gain:\n",
    "\n",
    "   Gini = 1/2 * Î£(Î£ |x_i - x_j|),\n",
    "\n",
    "\n",
    "\n",
    "# Q15. What is overfitting in decision trees and how can it be prevented?\n",
    "- Overfitting occurs when the decision tree learns the training data too well, capturing noise and losing generalization.\n",
    "Prevention techniques:\n",
    "Pruning (cutting branches)\n",
    "Setting max depth\n",
    "Minimum samples for split/leaf\n",
    "Cross-validation\n",
    "Using ensemble methods (Random Forest, Gradient Boosting)\n",
    "\n",
    "# Q16. What is pruning? And why is it necessary?\n",
    "- Pruning is the process of removing nodes from a decision tree to reduce complexity and improve generalization.\n",
    "\n",
    "Pre-pruning: Stop tree growth early based on conditions (max depth, min samples)\n",
    "\n",
    "Post-pruning: Remove nodes after the tree is fully grown\n",
    "\n",
    "It is necessary to avoid overfitting and improve model performance on unseen data.\n",
    "\n",
    "# Q17. What are the stopping criteria for a decision tree?\n",
    "- Maximum depth reached (max_depth)\n",
    "Minimum number of samples to split (min_samples_split)\n",
    "Minimum samples at leaf (min_samples_leaf)\n",
    "No further information gain\n",
    "All data points in a node belong to the same class\n",
    "\n",
    "\n",
    "# Q18. What is the role of max_depth, min_samples_split, and min_samples_leaf?\n",
    "- max_depth: Limits how deep the tree can go. Prevents overfitting.\n",
    "min_samples_split: Minimum samples required to split an internal node.\n",
    "min_samples_leaf: Minimum samples required to be at a leaf node.\n",
    "These are regularization parameters to control model complexity.\n",
    "\n",
    "# Q19. How does a decision tree handle missing values?\n",
    "- Surrogate Splits: Use alternative features that mimic the split of the original feature\n",
    "Ignore samples with missing values\n",
    "Imputation: Fill missing values before training\n",
    "Some implementations (like CART) handle missing values internally\n",
    "\n",
    "\n",
    "# Q20. What is the difference between supervised and unsupervised machine learning models?\n",
    "\n",
    "| Aspect     | Supervised Learning                     | Unsupervised Learning                 |\n",
    "| ---------- | --------------------------------------- | ------------------------------------- |\n",
    "| Labels     | Data has labeled outputs                | Data has no labels                    |\n",
    "| Purpose    | Predict output values                   | Discover hidden patterns or groupings |\n",
    "| Examples   | Classification, Regression              | Clustering, Dimensionality Reduction  |\n",
    "| Algorithms | Logistic Regression, Decision Tree, SVM | K-Means, PCA, Hierarchical Clustering |\n",
    "| Output     | Predictive models                       | Descriptive patterns                  |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6f535-784b-413e-ad4e-c7c6521ffc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
